\section{File Systems, Deadlock, Security}
{\bf Recap of address
translation}
page replacement algorithms need to avoid thrashing (it prevents useful work). Some solutions
to thrashing include: swapping (write out all pages of procerss and suspsend it) and the OOM killer daemon.
Previously, we assumed that the replacement algorithm chooses a victim page
when a \emph{new page needs to be brought in}. But these algorithms are too
costly to run on every page fault. Instead, we maintain a pool of free pages,
run replacement algorithm when pool is small, grab a page on frame from free
list on page fault, and Frames on free list can be rescued if virtual page is
referenced before reallocation.
{\bf Where to store page
tables } can store them in physical memory (no translation required) or in virtual memory.
{\bf Managing swap space} here we can use a raw disk partition, or use a large
file in FS. tradeoffs include choosing when the swap be allocated/freed.  {\bf
Address translation reduction} (1)  Read Access (Load)
  (2) Address goes to Translation Lookaside Buffer (TLB) in Memory
  Management Unit (MMU)
  (3) TLB does lookup using page number of address
  (4) Returns Page Table Entry (PTE) for the mapping of this address.
    \emph{Could Miss}
  (5)  TLB validates the PTE protection (allows read) \emph{Could Miss}
    (6)  PTE specifies which \emph{physical} frame holds the page
    (7) MMU combines physical frame and offset into a \emph{physical
    address}.
    (8) PTE specifies which \emph{physical} frame holds the page
    (9) MMU combines physical frame and offset into a \emph{physical
    address}.
    (10) MMU reads from that physical address and returns value to CPI.
  This is all done by hardware.
  If the TLB does not have a mapping, either:
    (1)  MMU loads PTE from page table in memory. (OS not involved in this)
    (2) OS has already set up the page tables so the hardware can access it
    directly.
    (3) Trap to OS (Software managed TLB)
    (4) OS does a lookup in page table and loads PTE into TLB.
    (5) Return from the exception, and retry memory address.
    (6) This is known as a \emph{minor page fault}. We have now created a
    PTE for the addr in the TLB
{\bf Implementatoin concerns} How do we determine how much memory to give each
process? We can use: (1) fixed space algorithms: when we reach space limit, do
local replacement. or (2) variable space algorithms, where space of a page
grows/shrinks dynamically (but one process can ruin it for the rest lol). or
(3) a working set model, where $WS(t,d) = \{ p \in {\tt Pages} \bar p {\text
referenced in } (t-d, t)$, where $t$ is time and $d$ is the working set window,
meausred in page references. The working set (WS) changes with program
locality, poor locality implies more page references, and it needs to be the
set of pages a process needs in mem to prevent faulting, so we dont run a
process unless the working set of that process is in memory. this is not
practical cuz dertermining $d$ is difficult.  {\bf Shared Memory} is used to
allow processes to share data using \emph{direct memory references}.  Here,
both page tables map to the same physical frame, and each PTE can have
different protecion values, and we need to update both PTEs when a page table
becomes valid.
{\bf Copy on Write}
Defers large copies as long as possible, in hopes to avoid them
altogether.uses   \emph{Shared mappings} instead. these are protected as read only.
{\bf File Systems}
File systems provide long term info storage. they need to (1) store large amounts of info that (2) needs to persist and (3) must be accessible.
The OS needs to manage the physical storage media, and enforce access
restriction, while the user only needs a convenient way of organizing
information.
We abstract secondary storage using \emph{files}, and we organize files
using \emph{directories}.
{\bf Files}
{\bf Operations } done on files: (1) Creation, (2) writin, (3) reading, (4) repositioning, (5) deleting (the whole file) (6) truncating (deleting contents of file)
The attributes of a file are stored in a suystem wide \emph{open-file
table}. The page is located at the index of its file descriptior (hence
\texttt{FILE\ *fd}). In this way we can do operations on a file.
{\bf Directories}
Directories provide a convenient naming interface for the file system,
which allows for the implementation to distinguish between local file
organization and physical memory placement on disk.
A directory is a usually unordered list of entries, which are names and
associated \emph{metadata}. Entries are usually sorted by the program
that reads the directory.
Directories are usually stored in files. In this way, you only need to
manage on kind secondary storage unit.
We can organize directories as a tree or a general acyclic graph.
Acyclic graphs allow for shared directories. We can implement this using
a linear list or a Hash table.
{\bf Sharing}
Sharing files is the basis for communications and synchronization.
Key issues: semantics of concurrent access, protection.
2 levels of internal tables: (1) Per-process table that stores the files that
process has open. (2) The entry of the per-process table points to an entry in
the system-wide open-file file table. (this gives process independent info).
General-purpose file systems support simple methods, like sequential
access (one byte at a time) and direct access (random access given a
number of bytes).
Database systems support more sophisticated methods, like indexed
accesses or record accesses.
{\bf File Links}
We can implement sharing by creating a \emph{link}. A link is a
directory entry that points to another file or subdirectory. We do this
with \emph{hard links} (second directy entry identical to first) or a
symbolic link (holds the true path to the linked file).
The issue with acyclic graphs is traversal, and updating permissions,
etc. is more difficult.
{\bf File System Implementation}
Define a \emph{block size} (like 4kb). The disk space is allocated in
granulatiry of blocks
Define a \emph{master block}, which determines the location of the root
directory. This is usually placed at a well known disk location, and
replicated across disk for reliability.
Define a \emph{free map} which determines which blocks are free. This is
usually a bitmap, with one bit per block on the disk. This is also
stored on disk and cached in memory for performance.
{\bf Disk Layout Strategies}
We develop a solutions to finding all the blocks for a file that spans multiple
blocks.  \emph{Contiguous Allocation} is like meomory, fast and simple, but
inflexible and causes fragemntation \emph{Linked/Changed structure}, where each
blocks points to the next, and the directory points to the first.
\emph{Indexed structure}, where each ``index block'' contains pointers to other
blocks. This handles randomness better, but may need multiple index blocks
linked together.
{\bf Inodes}
Inodes implement an indexed file structure for file.
All metadata is stored in inodes.
Inodes contain 15 block pointers.
Inodes are smaller than disk blocks.
The system spends a lot of time walking directory paths, and this is why
\emph{open} is separate from \emph{read/write}.
It's important to note that \texttt{inodes} are \emph{not} directories,
but they describe where on the disk the blocks for a file are placed.
\emph{Directory entries map file names \texttt{inodes}}
{\bf Disk I/O and File System
Optimization}
We use most of the disk space to store user data (\texttt{D}), while
leaving some space for metadata.
The file system needs to keep the info of each file in an inode struct
(the \texttt{I}s).
We keep track of which blocks are being used and which are free, so we
use a \emph{bitmap}, where each bit indicates if one block is free
(\texttt{0}) or in use (\texttt{1}). We can keep track of 32 blocks with
a 4kb data bitmap.
The superblock contains information about this particular file system, like (1) the type of file system,
(2) the num of inodes and data blocks, and (3) where the inode table begins.
The OS first reads the super block when mounting the file system, it
then attatches the volume to the file system tree with proper settings.
{\bf Unix inode Structure}
{\bf Pointer Based}
If an inode contains an array of 15 \emph{direct pointers} that point to
15 data blocks that belong to the file. Then the maximum file size
supported is (15 * 4KB) 60KB. (4kb page size)
But what if we have a file larger than 60kb?
{\bf Multi-level index with direct
pointers}
Direct pointers to disk blocks dont support large files, so we use an
\emph{indirect pointer}. The block points to pointers instead of user
data.
From the 15 pointers, we have an inode, so we use the first 14 as direct
pointers and the 15th as an indirect, so we can now support a file of up
to (4k * (14 + 1k) ) 4152kb.
If we want even bigger files, we use a \emph{double indirect pointer},
which is a pointer that points to a block full of indirect pointers
which point to blocks of direct pointers.
For the first 15 pointers, we use 13 as direct pointers, the 14th as an
indirect pointer, and the 15th as a double indirect pointer. So we can
now suppoer a file size of (4k * (13 + 1024 + 1024\^{}2)
)\texttt{4.00GB}
{\bf Extent Based}
Another approach is using \emph{extent-base} method. An \emph{extent} is
a disk pointer plus a length, so it allocates a few blocks in a row. Now
instead of having a pointer to every block, we only need a pointer to
every several blocks (every extent). This is less flexible than the
pointer based approach, but the advantage is that we need less metadata.
{\bf Linked Based}
Instead of pointers to every block, just point to the first data block,
and the first point to the second, etc. This isn't good if we want to
access the last block of a large file.
We can use a \emph{file allocation table}, indexed by address of data
block, this is faster for finding a block.
{\bf File System Optimization}
{\bf Cashing}
Filer operations incur a lot of disk i/o. For the file system to perform
reasonably well, it does caching.
{\bf File Buffer Cache}
Apps exhibit significant locality for reading and writing files. Cache
file blocks in memory to capture locality. This is called a \emph{File
Buffer Cache}.
Cache is system wide, and reading from it makes a disk perform like
memory. Significant reuse is for spatial and temporal locality.
We cache inodes, directory entries, disk blocks for `hot' files.
Static Partitioning
At boot time, allocate a fixed-size cache in memory (typically 10\%).
But this can be wasteful if the file doesn't need all this.
Dynamic Partitioning
Integrate virtual memory pages and file system pages into a unified page
cache.
{\bf Caching Writes}
We use a \emph{buffer} to buffer writes. This lets us combine multiple
writes into one write (e.g.~updating multiple bits of the inode bitmap),
imporve performance (shedule the writes so that they happen
sequentially), and can help us avoid some writes (like changing a bit
from 0 to 1 to 0 (undos)).
Unofrtunately, buffering (and caching) sacrifice the durability of data.
If a crash occurs, and the buffer has not been written to disk, we lose
everything :'(, but if we sync to the disk more frequently, we loose
spped.
{\bf Disk Scheduling}
High-level disk characteristics yield two {\bf goals }:
(1) Closeness: Reduce seek times by putting related things together
(2) Amoritization: grab a lot of useful data when in a position
{\bf Allocation strategies:}
Disk perform best when seeks are reduced and large transfers are used.
We can make disks perform best by using scheduling requests and
allocating data ``close together''.
{\bf Original Unix File
System}
This is easy to implement, however performs poorly on the disk. On a new
FS, blocks are allocated sequentially, however, and older FS will have
gaps due to files being delted, and in an agin file system, data blocks
can be allocated far from each other. Framgentation of aging FS
=\textgreater{} more seeking.\\
Since inodes are located at the beggining of the disk, they are far from
the data, but every time we traverse a file path we need to inpspect the
inodes first.
{\bf BSD FFS}
The BSD Fast File System (FFS) uses a cylinder group to address
placement problems.
Cylinder groups provide \emph{closeness}, and so free space is scattered
across cylinders, and 10\% of the disk is reserved to keep the disk
partially free at all times.
{\bf Algorithms}
  (1) FCFS
  first come first serve, this is reasonable for low load, but has the
  longest wait time for long requests
  (2)
  SSTF
  shortest seek time first, this tries to minimize arm movements (seek
  time) and maximize request rate, and favors the middle blocks
  (3) SCAN
  this is an elevator technique, it services requests in one direction
  until done, then reverse
  (4) C-SCAN
  Like SCAN, but only goes in one direction, like a typewriter
  (5) LOOK / C-LOOK
  Like SCAN / C-SCAN, but does not go the full width of the disk.

% Lecture 11

{\bf file system comparisons} table layout = (a): index structure, (b) index
structure granularity, (c) free space mgmnt, (d) locality hueristcis.  FAT uses
(a) LL, (b) block, (c) FAT array, (d) defragmentation.  FFS uses (a) Tree
(inodes), (b) block, (c) bitmap, (d) block groups, reserved space.  NTFS uses
(a) tree (extents), (b) extent, (c) bitmap, (d) best fit defragmentation.  {\bf
FAT} has poor random access, poor locality, and limited file metadata and
access control.  
{\bf FFS} needs metadata updates to be {\it synchronous }, and
file system operations affect mulitple metadata block.  because it is
synchronous, we need to worry about crashes, here are solutions: (1) {\bf fsck}
a post crash process that scans the file system structure and restores
consistency, but cannot fix everything (like datablock consistency); checks
superblock, free blocks, inode state, inode links, duplicates, bad blocks, and
dir checks (. and .. first entries, etc). (2) {\bf journaling} this is
write-ahead-logging. (1) write data, wait until complete, (2) meta data journal
write, (3) metadata journal commit, (4) checkpoint metadata, (5) free. 
% section
{\bf log structured file system LSF} doesnt care about reads, assumes writes
pose a bigger i/o penality, treats storage as circular log. this has simpler
crash recovery, and writes are batched into large chunks, but the assumptoin is
not always true, since reads are slower on HDDs. finding inodes is difficult,
since they're scattered, so it uses an imap (also placed in chunks next to new
info), and a checkpoint region (CR). Garbage collection is tricky.  {\bf
redundancy} is used to prevent data loss (in case we have disk failures). {\bf
RAID} (redundant array of independent disks) keeps a mirror of the disk. also,
{\bf parity information}, XOR each bit from 2 drives, and store checksum on 3rd
drive.  {\bf SSD algorithms} wear levelling: always write to new location, keep
a map from logical FS block num to current SSD block and page location; garbage
collection: reclaiming stale pages and creating empty erased blocks; RAID5 (w/
parity checking) striping across i/o channles to multiple nand chips.

% L12
Non deadlock bugs include atomicity violations (use locks) and order violation
(enforce ordering or extra checks). For deadlock to occur, mutual exclusion,
hold and wait, and no preemption are necessary, and circular wait is sufficient
for deadlock. Breaking one of the conditions means you don't have deadlock. For
mutual exclusion, we can do some operations atomically, like an atomic increment ({\tt do \{ int
old = *value; \} while (CompareAndSwap(value, old, old + amount) == 0); }.
We can also break {\bf hold and wait}, where a process holds until it gets all
of the resources, but this means that it could wait a while, and it limits
concurrency. or we can break {\bf no preemption}, by forcibly removing a
resource from one process and giving it to another (need to save state of
losing process, or rollback, or something like that). or we could break {\bf
circular wait} by assigning a linear ordering to resource types and requiring
that a process holding a resource of one type can only request resources that
follow in the ordering. two startegies for {\bf avoiding} deadlock: (1) don't
start a process if max resource reqs + max needs of all processes > system
resources. (2) don't grant an individual resource request if any future
resource allocation path leads to deadlock. {\bf DEADLOCK AVOIDANCE ALGORITHM}
for every resource request: (1) can request be granted? if no then block until
grant is possible, else (2) assume request is granted, update state, then (3)
if state is safe, continue, else restore old state and block process until its
safe to grant. {\bf restrictions on avoidance} (1) max resource requirements
for each process must be known in advance, (2) processes must be independent,
(3) fixed number of resources to allocate. {\bf DEADLOCK RECOVERY} selectively
kill deadlocked processes until cycle is broken (re-run detection algo after
each kill).  {\bf ostrich algo } works because resoures are finite, processes
wait if a resource they need is unavailable, resources may be held by other
waiting processes.
{\bf concurrent transactions} soln (1): all transactions execute in a critical
section w/ a single common lock (this limits concurrency unnecessarily). (2)
allow ops from multiple transactoins to overlap, as long as they dont conflict.
{\bf ensuring serializability} use two-phase locking: individual data has their
own lock, and each transaction has a growing phase and a shrinking phase. This
does not guarantee deadlock-free, (cuz hold and wait), soln: abort and retry
transaction if any lock is unavailable.
